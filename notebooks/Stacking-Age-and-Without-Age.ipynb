{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Age and without Age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/raw/train.csv')\n",
    "dfX = df.drop('Survived', axis=1)\n",
    "dfy = df.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../titansurv/pipeline/data_pipeline.py:41: UserWarning: Please set the data first using set_data method!\n",
      "  warnings.warn('Please set the data first using set_data method!')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from titansurv.pipeline import pipeline1 as dp1\n",
    "from titansurv.utils import print_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 1 Age with Mean Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The following were the preprocessing steps used: \n",
       "1. **Embarked**: Dropped NA rows and applied OneHotEncoding\n",
       "2. **Age** : Applied Mean Imputation and Mean Normalization\n",
       "3. **Fare**: Mean Normalization\n",
       "4. **Sex**: OneHotEncoding\n",
       "5. **Name**: Categorised into ['Mr', 'Mrs', 'Miss', 'Master', 'Special']<br/> \n",
       "    5.1 Rename [Mlle, Ms] -> Miss      \n",
       "    5.2 Rename [Mme] -> Mrs     \n",
       "    5.3 Put the Rest -> Special     \n",
       "    Then performed OneHotEncoding\n",
       "6. **Ticket** categorized into [1: numeric, 0: else] <br/>\n",
       "    6.1 Remove special characters but not space <br/>\n",
       "    6.2 Replace numeric strings by 'numeric' <br/>\n",
       "    6.3 Split on space and keep the first item <br/>\n",
       " Then applied binarizer for [1: numeric, 0: else]\n",
       "7. **SibSp** binned into [0, 1, >1] and applied OneHotEncoding\n",
       "8. **Parch** binned into [0, 1, >1] and applied OneHotEncoding\n",
       "\n",
       "Tuned ML model: **RandomForestClassifier** using GridSearchCV"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dp1.get_description(markdown=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 2 Drop Age column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data = dp1.preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n_jobs',\n",
      " 'remainder',\n",
      " 'sparse_threshold',\n",
      " 'transformer_weights',\n",
      " 'transformers',\n",
      " 'verbose',\n",
      " 'enc',\n",
      " 'imp_scaler',\n",
      " 'scaler',\n",
      " 'pre_Name',\n",
      " 'pre_Cabin',\n",
      " 'pre_Ticket',\n",
      " 'Pre_SibSp',\n",
      " 'Pre_Parch',\n",
      " 'enc__categories',\n",
      " 'enc__drop',\n",
      " 'enc__dtype',\n",
      " 'enc__handle_unknown',\n",
      " 'enc__sparse',\n",
      " 'imp_scaler__memory',\n",
      " 'imp_scaler__steps',\n",
      " 'imp_scaler__verbose',\n",
      " 'imp_scaler__imp',\n",
      " 'imp_scaler__scaler',\n",
      " 'imp_scaler__imp__add_indicator',\n",
      " 'imp_scaler__imp__copy',\n",
      " 'imp_scaler__imp__fill_value',\n",
      " 'imp_scaler__imp__missing_values',\n",
      " 'imp_scaler__imp__strategy',\n",
      " 'imp_scaler__imp__verbose',\n",
      " 'imp_scaler__scaler__copy',\n",
      " 'imp_scaler__scaler__with_mean',\n",
      " 'imp_scaler__scaler__with_std',\n",
      " 'scaler__copy',\n",
      " 'scaler__with_mean',\n",
      " 'scaler__with_std',\n",
      " 'pre_Name__memory',\n",
      " 'pre_Name__steps',\n",
      " 'pre_Name__verbose',\n",
      " 'pre_Name__featurize',\n",
      " 'pre_Name__enc',\n",
      " 'pre_Name__featurize__accept_sparse',\n",
      " 'pre_Name__featurize__check_inverse',\n",
      " 'pre_Name__featurize__func',\n",
      " 'pre_Name__featurize__inv_kw_args',\n",
      " 'pre_Name__featurize__inverse_func',\n",
      " 'pre_Name__featurize__kw_args',\n",
      " 'pre_Name__featurize__validate',\n",
      " 'pre_Name__enc__categories',\n",
      " 'pre_Name__enc__drop',\n",
      " 'pre_Name__enc__dtype',\n",
      " 'pre_Name__enc__handle_unknown',\n",
      " 'pre_Name__enc__sparse',\n",
      " 'pre_Cabin__memory',\n",
      " 'pre_Cabin__steps',\n",
      " 'pre_Cabin__verbose',\n",
      " 'pre_Cabin__featurize',\n",
      " 'pre_Cabin__enc',\n",
      " 'pre_Cabin__featurize__accept_sparse',\n",
      " 'pre_Cabin__featurize__check_inverse',\n",
      " 'pre_Cabin__featurize__func',\n",
      " 'pre_Cabin__featurize__inv_kw_args',\n",
      " 'pre_Cabin__featurize__inverse_func',\n",
      " 'pre_Cabin__featurize__kw_args',\n",
      " 'pre_Cabin__featurize__validate',\n",
      " 'pre_Cabin__enc__categories',\n",
      " 'pre_Cabin__enc__drop',\n",
      " 'pre_Cabin__enc__dtype',\n",
      " 'pre_Cabin__enc__handle_unknown',\n",
      " 'pre_Cabin__enc__sparse',\n",
      " 'pre_Ticket__memory',\n",
      " 'pre_Ticket__steps',\n",
      " 'pre_Ticket__verbose',\n",
      " 'pre_Ticket__featurize',\n",
      " 'pre_Ticket__binarizer',\n",
      " 'pre_Ticket__featurize__accept_sparse',\n",
      " 'pre_Ticket__featurize__check_inverse',\n",
      " 'pre_Ticket__featurize__func',\n",
      " 'pre_Ticket__featurize__inv_kw_args',\n",
      " 'pre_Ticket__featurize__inverse_func',\n",
      " 'pre_Ticket__featurize__kw_args',\n",
      " 'pre_Ticket__featurize__validate',\n",
      " 'pre_Ticket__binarizer__accept_sparse',\n",
      " 'pre_Ticket__binarizer__check_inverse',\n",
      " 'pre_Ticket__binarizer__func',\n",
      " 'pre_Ticket__binarizer__inv_kw_args',\n",
      " 'pre_Ticket__binarizer__inverse_func',\n",
      " 'pre_Ticket__binarizer__kw_args',\n",
      " 'pre_Ticket__binarizer__validate',\n",
      " 'Pre_SibSp__memory',\n",
      " 'Pre_SibSp__steps',\n",
      " 'Pre_SibSp__verbose',\n",
      " 'Pre_SibSp__binner',\n",
      " 'Pre_SibSp__enc',\n",
      " 'Pre_SibSp__binner__accept_sparse',\n",
      " 'Pre_SibSp__binner__check_inverse',\n",
      " 'Pre_SibSp__binner__func',\n",
      " 'Pre_SibSp__binner__inv_kw_args',\n",
      " 'Pre_SibSp__binner__inverse_func',\n",
      " 'Pre_SibSp__binner__kw_args',\n",
      " 'Pre_SibSp__binner__validate',\n",
      " 'Pre_SibSp__enc__categories',\n",
      " 'Pre_SibSp__enc__drop',\n",
      " 'Pre_SibSp__enc__dtype',\n",
      " 'Pre_SibSp__enc__handle_unknown',\n",
      " 'Pre_SibSp__enc__sparse',\n",
      " 'Pre_Parch__memory',\n",
      " 'Pre_Parch__steps',\n",
      " 'Pre_Parch__verbose',\n",
      " 'Pre_Parch__binner',\n",
      " 'Pre_Parch__enc',\n",
      " 'Pre_Parch__binner__accept_sparse',\n",
      " 'Pre_Parch__binner__check_inverse',\n",
      " 'Pre_Parch__binner__func',\n",
      " 'Pre_Parch__binner__inv_kw_args',\n",
      " 'Pre_Parch__binner__inverse_func',\n",
      " 'Pre_Parch__binner__kw_args',\n",
      " 'Pre_Parch__binner__validate',\n",
      " 'Pre_Parch__enc__categories',\n",
      " 'Pre_Parch__enc__drop',\n",
      " 'Pre_Parch__enc__dtype',\n",
      " 'Pre_Parch__enc__handle_unknown',\n",
      " 'Pre_Parch__enc__sparse']\n"
     ]
    }
   ],
   "source": [
    "print_params(preprocess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from titansurv.preprocessing import modify_transformer_est\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataPipeline"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp2 = clone(dp1)\n",
    "dp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp2.preprocess_data = modify_transformer_est(preprocess_data, impute_scaler='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Combine the predictions from the two using a StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp1.data = None\n",
    "dp2.data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Survived'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp1.ycol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_pipe = StackingClassifier([dp1], meta_classifier=LogisticRegression())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:titansurv] *",
   "language": "python",
   "name": "conda-env-titansurv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
